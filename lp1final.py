# -*- coding: utf-8 -*-
"""lp1final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DikDTouH4TGBVhpT3C0dqFw7ppLhwh2k
"""

# --- FINAL MAIN SCRIPT FOR PHASE A (JSON Loading) ---
# Phase A
import json
import os
import sys
# from google.colab import drive

print("--- Phase A: Data Ingestion from JSON Files (Final Version) ---")

# # Mount Google Drive
# if not os.path.isdir('/content/drive/MyDrive'):
#     print("\nMounting Google Drive...")
#     drive.mount('/content/drive')
# else:
#     print("\n✅ Google Drive already mounted.")

# --- IMPORTANT: Set your folder path here ---
# Use a raw string to avoid Windows escape issues (e.g., \a)
folder_path = r'D:\Projects\LawPal\ai-lawyer-chat\my-ai-lawyer-app\lawpal json'  # <-- Ensure this is your correct folder path

if not os.path.isdir(folder_path):
    print(f"❌ ERROR: The folder path '{folder_path}' does not exist.")
else:
    all_final_chunks = []
    chunk_id_counter = 1

    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]
    print(f"\nFound {len(json_files)} JSON files to process.")

    for file_name in json_files:
        file_path = os.path.join(folder_path, file_name)
        print(f"\nProcessing: {file_name}...")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            created_count = 0
            # Iterate through each item (section/article) in the JSON file
            for item in data:
                # This logic handles the different key names across all your JSON files
                section_num = item.get('section', item.get('Section', item.get('article')))
                title = item.get('title', item.get('section_title', ''))
                description = item.get('description', item.get('section_desc', ''))

                # Skip empty or invalid entries
                if not (section_num and description):
                    continue

                # *** KEY IMPROVEMENT (from our last fix) ***
                # We combine the title and the description into the chunk_text.
                # This enriches the data and makes it much easier to find in the search phase.
                enriched_text = f"{title}. {description}"

                all_final_chunks.append({
                    "chunk_id": f"chunk_{chunk_id_counter}",
                    "source_document": file_name,
                    "document_type": "statute",
                    "section_id": f"{'article' if 'article' in item else 'section'} {section_num}", # Correctly label as article or section
                    "chunk_text": enriched_text
                })
                chunk_id_counter += 1
                created_count += 1

            print(f"  - Success: Created {created_count} chunks.")

        except Exception as e:
            print(f"  - ❌ ERROR processing {file_name}: {e}")

    # Save the final, clean dataset
    output_filename = 'processed_legal_data.json'
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(all_final_chunks, f, indent=2)

    print(f"\n✅✅✅ Processing complete. Knowledge base rebuilt successfully.")
    print(f"Total chunks created: {len(all_final_chunks)}")
    print(f"All data saved to '{output_filename}'. You are ready for Phase B.")

# --- Final Phase B: Indexing & Retrieval Engine ---

# Step 1: Ensure required libraries are installed in your environment
# (Installation should be done via pip in your terminal, not via notebook magics.)

import json
import numpy as np
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.utils import embedding_functions
from rank_bm25 import BM25Okapi  # <-- Added missing import

print("--- Initializing Phase B: Retrieval Engine ---")

# Step 2: Load the processed data from Phase A
data_filename = 'processed_legal_data.json'
try:
    with open(data_filename, 'r', encoding='utf-8') as f:
        all_chunks = json.load(f)
    print(f"✅ Successfully loaded {len(all_chunks)} chunks from '{data_filename}'.")
except FileNotFoundError:
    print(f"❌ ERROR: '{data_filename}' not found. Please run the Phase A script first.")
    sys.exit(1)

# Step 3: Initialize the powerful embedding model
print("\n1. Initializing embedding model (BAAI/bge-large-en-v1.5)...")
embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')
print("   - Model loaded successfully.")

# Step 4: Initialize and build ChromaDB Vector Database
client = chromadb.Client()
collection_name = "indian_legal_docs_final"
if collection_name in [c.name for c in client.list_collections()]:
    client.delete_collection(name=collection_name)
# Configure Chroma to use the same embedding model for text & queries
ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='BAAI/bge-large-en-v1.5')
collection = client.create_collection(name=collection_name, embedding_function=ef)

print("\n2. Embedding and indexing all chunks. This will take some time...")
documents = [chunk['chunk_text'] for chunk in all_chunks]
metadatas = [{'source': chunk['source_document'], 'section': chunk['section_id']} for chunk in all_chunks]
ids = [chunk['chunk_id'] for chunk in all_chunks]
batch_size = 32
for i in range(0, len(documents), batch_size):
    collection.add(ids=ids[i:i+batch_size], documents=documents[i:i+batch_size], metadatas=metadatas[i:i+batch_size])
print("✅ Vector Database created.")

# Step 5: Initialize and build BM25 Lexical Index
print("\n3. Initializing Lexical/BM25 Index...")
tokenized_corpus = [doc.split(" ") for doc in documents]
bm25 = BM25Okapi(tokenized_corpus)
print("✅ BM25 Index created.")

# Step 6: Define Hybrid Search Function
def hybrid_search(query, top_k=20): # <-- Set default to 20 as discussed
    print(f"\n   - Executing hybrid search for query: '{query}'")
    vector_results = collection.query(query_texts=[query], n_results=top_k)
    vector_ids = vector_results['ids'][0]
    print(f"     - Vector search found {len(vector_ids)} results.")

    tokenized_query = query.split(" ")
    bm25_scores = bm25.get_scores(tokenized_query)
    top_n_indices = np.argsort(bm25_scores)[::-1][:top_k]
    bm25_ids = [ids[i] for i in top_n_indices]
    print(f"     - BM25 search found {len(bm25_ids)} results.")

    fused_ids = list(dict.fromkeys(vector_ids + bm25_ids))
    print(f"     - Fused results to {len(fused_ids)} unique chunks.")

    fused_results = [chunk for chunk in all_chunks if chunk['chunk_id'] in fused_ids]
    return fused_results

print("\n✅✅✅ PHASE B READY: Hybrid Search engine is operational.")

# --- Final Phase C: Reranking Engine ---
import json
import re
from sentence_transformers.util import cos_sim

print("--- Initializing Phase C: Reranking Engine ---")

def advanced_legal_reranker(query, search_results, embedding_model):
    """
    Final reranker with a hybrid score of heuristics, semantic relevance, entity matching, AND keyword boosting.
    """
    print("\n   - Applying Final Legal Reranker...")

    # --- Part 1: Initial Set up ---
    priority_docs = [
        "Constitution of India.json",
        "Indian Penal Code, 1860.json"
    ]

    # --- Part 2: Semantic Similarity Scoring ---
    query_embedding = embedding_model.encode(query)
    chunk_texts = [res['chunk_text'] for res in search_results]
    chunk_embeddings = embedding_model.encode(chunk_texts)
    similarities = cos_sim(query_embedding, chunk_embeddings)[0]

    # --- Part 3: Entity Matching Logic ---
    query_doc_match = re.search(r'(indian evidence act|indian penal code|code of criminal procedure|code of civil procedure|constitution of india|motor vehicles act)', query, re.IGNORECASE)
    query_sec_match = re.search(r'(section|article) (\d+\w*)', query, re.IGNORECASE)
    query_doc = query_doc_match.group(0).lower() if query_doc_match else None
    query_sec_num_str = query_sec_match.group(2) if query_sec_match else None

    if query_doc or query_sec_num_str:
        print(f"     - Query entities found: Document='{query_doc}', Section/Article='{query_sec_num_str}'")

    # --- Part 4: Keyword Boosting Logic ---
    keywords = ["insurance", "penalty", "offense", "fine", "imprisonment", "uninsured", "expired", "alcohol", "blood"]
    query_keywords = [word for word in query.lower().split() if word in keywords]
    if query_keywords:
        print(f"     - Query keywords found: {query_keywords}")

    # --- Part 5: Scoring and Reranking Loop ---
    final_results = []
    for i, chunk in enumerate(search_results):
        # Heuristic Score
        heuristic_score = 1.0
        if chunk['source_document'] in priority_docs:
            heuristic_score = 1.5

        # Semantic Score
        semantic_score = similarities[i].item()

        # Entity Match Boost
        exact_match_boost = 0.0
        try:
            chunk_doc = chunk['source_document'].lower()
            match = re.search(r'\d+', chunk['section_id'])
            if match:
                chunk_sec_num = int(match.group())
                if query_doc and query_sec_num_str and query_doc in chunk_doc and str(chunk_sec_num) == query_sec_num_str:
                    exact_match_boost = 10.0
                    print(f"     - PERFECT MATCH: Boosting score for {chunk['source_document']} - {chunk['section_id']}")
        except: pass

        # Keyword Boost
        keyword_boost = 0.0
        chunk_text_lower = chunk['chunk_text'].lower()
        for keyword in query_keywords:
            if keyword in chunk_text_lower:
                keyword_boost += 0.1

        # Final weighted score
        weight_heuristic = 0.1
        weight_semantic = 0.7
        weight_keyword = 0.2
        base_score = (heuristic_score * weight_heuristic) + (semantic_score * weight_semantic) + (keyword_boost * weight_keyword)
        final_score = base_score + exact_match_boost

        final_results.append({
            "final_score": round(final_score, 4),
            "chunk_data": chunk
        })

    final_results.sort(key=lambda x: x['final_score'], reverse=True)
    print("   - Reranking complete.")
    return final_results

print("\n✅✅✅ PHASE C READY: Advanced Reranker function is defined and operational.")

# --- FINAL POLISHED EXECUTION CELL (with comments) ---

# Step 1: Ensure 'groq' and 'tiktoken' are installed in your environment via pip.

# Step 2: Import all required libraries
import os
import json
import re
from groq import Groq
# Load GROQ_API_KEY from .env or OS environment (no Colab dependency)
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass
import tiktoken
from sentence_transformers.util import cos_sim
try:
    from IPython.display import display, Markdown  # Pretty output when available
except Exception:
    def display(x):
        print(str(x))
    class Markdown(str):
        pass

print("--- RAG AI Lawyer: Initializing Final Polished Execution Engine ---")

# Step 3: Define Helper Functions (Token Counter and Updated Prompter)

# Initialize a tokenizer that matches Llama models for accurate token counting.
tokenizer = tiktoken.get_encoding("cl100k_base")

def count_tokens(text):
    """Counts the number of tokens in a given text using the tiktoken library."""
    return len(tokenizer.encode(text))

def build_polished_prompt(query, reranked_chunks, token_budget):
    """
    This function builds the final, detailed prompt that will be sent to the AI.
    It includes the system instructions, the user's query, and dynamically packs
    the most relevant legal context to fit within a specified token budget.
    """
    # This is the main set of instructions for the AI. It controls its personality,
    # tone, and the rules it must follow, like citing sources.
    system_prompt = """
**SYSTEM PROMPT — AI LAWYER CHAT (Frontend Version)**

You are a professional Indian Legal AI Assistant. Answer the user's legal questions strictly based on the provided sources (Acts, Rules, or Case Law excerpts).

### Guidelines:

1. **Source-Based Responses**
   - Use only the information from the provided sources.
   - Do not invent or assume facts.
   - If insufficient information is available, respond:
     "The available legal documents do not contain sufficient information to answer this question."

2. **Legal Reasoning**
   - Identify relevant Sections or Clauses and explain why they apply.
   - Integrate multiple sources smoothly if necessary.

3. **Citation Style**
   - Cite naturally: “Under Section 13B of the Hindu Marriage Act, 1955…”  
   - Avoid numeric tags like [Source 1] or URLs.

4. **Tone & Style**
   - Professional, clear, and readable.
   - Avoid long academic lists or repetitive wording.
   - Keep answers concise for chat display, but legally precise.

5. **Confidence**
   - Optionally, include a confidence rating at the end:  
     *Confidence: X/10 — based on the completeness of the provided legal sources.*

6. **Applicability**
   - This prompt works for any Indian legal dataset (e.g., Motor Vehicles Act, IPC, CPC, Family Law, Constitution).

Remember: You are a legal reasoning assistant, not a general explainer. Focus on interpreting statutory language clearly for the user.
"""


    # Calculate the initial number of tokens used by the instructions and the query.
    prompt_header_tokens = count_tokens(system_prompt + query)
    current_tokens = prompt_header_tokens
    context_str = ""
    source_count = 0

    print("   - Packing context within token budget...")
    # This loop intelligently adds the best search results (reranked_chunks) to the prompt one by one,
    # stopping when the total token count gets close to our budget. This prevents API errors.
    for item in reranked_chunks:
        chunk_data = item['chunk_data']
        next_chunk_str = f"Source {source_count + 1}:\n"
        next_chunk_str += f"  Document: {chunk_data['source_document']}\n"
        next_chunk_str += f"  Section: {chunk_data['section_id']}\n"
        next_chunk_str += f"  Text: {chunk_data['chunk_text']}\n\n"

        chunk_tokens = count_tokens(next_chunk_str)

        if current_tokens + chunk_tokens <= token_budget:
            # If there's room, add the chunk to our context
            context_str += next_chunk_str
            current_tokens += chunk_tokens
            source_count += 1
        else:
            # If the next chunk is too big, stop here.
            print(f"   - Token budget reached. Packed {source_count} sources.")
            break

    # Assemble the final prompt with all the pieces.
    final_prompt = f"{system_prompt}\n---\n**Sources:**\n{context_str}---\n**User's Question:**\n{query}"

    return final_prompt, current_tokens

# Step 4: Set up the Groq client
# Load from environment (.env supported if python-dotenv is installed)
api_key = os.getenv('GROQ_API_KEY')
client = None
if not api_key:
    print("⚠️ GROQ_API_KEY not set. Skipping model generation; retrieval/reranking will still run.")
else:
    try:
        client = Groq(api_key=api_key)
        print("✅ Groq client initialized successfully.")
    except Exception as e:
        print(f"❌ ERROR: Could not initialize Groq client: {e}")
        client = None

# --- Step 5: Define Query and Execute Full Pipeline ---

# --- IMPORTANT: THIS IS THE ONLY PART YOU NEED TO EDIT TO ASK A NEW QUESTION ---
query = "A person is found driving a motor vehicle without holding a valid driving licence. The vehicle is stopped by the traffic police, and the driver is penalized under the Motor Vehicles Act, 1988. Which sections of the Act are applicable here, and what is the reasoning behind this penalty?"
# --------------------------------------------------------------------------------

display(Markdown(f"### Executing query: *'{query}'*"))

try:
    # --- Execute Phase B Logic (Retrieval) ---
    # Call the hybrid_search function (defined in your Phase B cell) to find relevant documents.
    print("\n1. Retrieving and Reranking context...")
    fused_results = hybrid_search(query=query, top_k=20)

    # --- Execute Phase C Logic (Reranking) ---
    # Call the advanced_legal_reranker (defined in your Phase C cell) to prioritize the search results.
    final_reranked_chunks = advanced_legal_reranker(
        query=query,
        search_results=fused_results,
        embedding_model=embedding_model
    )
    print("   - Context successfully retrieved and reranked.")

    # --- Execute Phase D Logic (Prompting & Generation) ---
    print("\n2. Building polished prompt and generating answer...")
    TOKEN_BUDGET = 8000 # Sets a safe limit for our prompt size.

    # Call our prompter function to build the final prompt with the best context.
    final_prompt, total_tokens = build_polished_prompt(
        query,
        final_reranked_chunks,
        TOKEN_BUDGET
    )
    print(f"   - Prompt built successfully. Final token count: {total_tokens}")

    # Send the final prompt to the Llama 3.3 model via the Groq API.
    if client is None:
        print("⚠️ Skipping model generation because Groq client is not configured. Set GROQ_API_KEY in .env to enable.")
    else:
        print(f"   - Calling 'llama-3.3-70b-versatile' on Groq...")
        chat_completion = client.chat.completions.create(
            messages=[{"role": "user", "content": final_prompt}],
            model="llama-3.3-70b-versatile",
        )

        # --- Step 6: Display the Final Answer ---
        # Print the AI's response in a clean, readable format.
        display(Markdown("--- \n### Final AI-Generated Answer"))
        display(Markdown(chat_completion.choices[0].message.content))

except NameError as e:
    print(f"\n❌ ERROR: A required variable or function is not defined: {e}")
    print("   Please ensure you have run the main cells for Phase A, B, and C in this session first.")
except Exception as e:
    print(f"\nAn unexpected error occurred: {e}")